---
title: "Homework 3"
author: "Lincole Jiang"
date: 'October 15th, 2022'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(readr)
# install.packages("devtools")
# devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Due date

Due: October 15 at 11:59pm. 

### Points

| Problem         | Points    |
|:--------------- |:--------- |
| Problem 0       | 20        |
| Problem 1       | --        |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |


### Problem 0

This “problem” focuses on the structure of submission, especially the use git and GitHub for reproducibility, R Projects to organize work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for files.

### Problem 1

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```


### Problem 2

For this problem, we look at the accelerometer data collected on a 63 year-old male with BMI 25. Firstly, we import and wrangle the data. More specifically, we create a new factor variable wkdy_vs_wknd to indicate whether the data is from a weekday or a weekend; use pivot_longer to create variables minute and activity that indicates the minute at which the activity is recorded and the activity count during that particular minute interval, respectively, and makes sure each variable is of the most reasonable variable class.

```{r}
accel_df <- read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(cols = starts_with("activity"),
               names_to = "minute",
               names_prefix = "activity.",
               values_to = "activity") %>%
  mutate(wkdy_vs_wknd = ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
         minute = as.integer(minute),
         wkdy_vs_wknd = as.factor(wkdy_vs_wknd)) 
```

Now, we provide a description for data set accel. This data is collected on a 63 year-old male with BMI 25 over the span of 5 weeks, who was admitted to the Cardiac Care Center of CUMC and diagnosed with congestive heart failure. More specifically, there are `r ncol(accel_df)` variables and `r nrow(accel_df)` observations; and below provides a description for each variable. 

Variable *week* indicates the week for which the observation is made since the beginning of the study. It is a `r class(accel_df$week)` variable. It takes values from `r min(accel_df$week)` to `r max(accel_df$week)`.

Variable *day_id* indicates the day for which the observation is made since the beginning of the study. It is a `r class(accel_df$day_id)` variable and takes values from `r min(accel_df$day_id)` to `r max(accel_df$day_id)`.

Variable *day* indicates the day of the week for which the observation is made. It is a `r class(accel_df$day)` variable.

Variable *minute* indicates the minute for which the observation is made. It is a `r class(accel_df$minute)` variable.

Variable *activity* indicates activity counts measured in the duration of the one-minute interval of the particular observation. It is a `r class(accel_df$activity)` variable and its range is from `r min(accel_df$activity)` to `r max(accel_df$activity)`.

Variable *wkdy_vs_wknd* indicates whether the observation was recorded on a weekday or on a weekend. It is a `r class(accel_df$wkdy_vs_wknd)` variable.

Now, we turn our attention to the total activity over the day and by week. To do this, we aggregate across minutes to create a total activity variable for each day and create a table showing the totals.

```{r}
accel_act <- accel_df %>% 
  group_by(day, week) %>%
  summarize(total_activity = sum(activity)) 

accel_act %>% 
  pivot_wider(
    names_from = day, 
    values_from = total_activity) %>%
  knitr::kable() # to show the table
```

To explore the trends associated with this data set, we create a ggplot where x-axis is week, y-axis is activity, and the color indicates day.

```{r}
accel_act %>% 
  ggplot(aes(x = week, y = total_activity, color = day)) +
  geom_smooth()
```

As such, we could perhaps conclude that week 4 has the lowest activity, and Monday and Friday are where activity is the most variable in all weeks.

Finally, we make a single-panel plot showing the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity)) +
  geom_point(aes(color = day))
```

To describe the graph, it seems like the activity before ~300 minute has no ostensibly high counts, whereas exceeding ~300 minute there seems to be a general increase in activity count as well as very high counts compared to the former part. The plot can be said to be skewed to the left. There doesn't seem to be much variance in activity via day of the week, on the other hand.


### Problem 3

For this problem, we explore "NY NOAA" data. We first load the data.

```{r}
library(p8105.datasets)
data("ny_noaa")
```

First, we provide a brief description of the data. Firstly, data has `r ncol(ny_noaa)` variables and `r nrow(ny_noaa)` observations. Specifically, *id* is a `r class(ny_noaa$id)` variable that describes Weather station ID; *date* is a `r class(ny_noaa$date)` variable that describes the date of the observation; *prcp* is a `r class(ny_noaa$id)` variable that describes the precipitation in tenths of mm; *snow* is a `r class(ny_noaa$snow)` variable indicating snowfall in mm; *snwd* is a `r class(ny_noaa$snwd)` variable indicating snow depth in mm; and *tmax* and *tmin* are `r class(ny_noaa$tmax)` variables indicating the maximum and minimum temperature in tenths of degrees C, respectively. On the other hand, missing value is a problem for this data set. Of the `r nrow(ny_noaa)` recorded by this data set, the number of missing values for id and date are both `r sum(is.na(ny_noaa$date))`, for prcp is `r sum(is.na(ny_noaa$prcp))`, for snow is `r sum(is.na(ny_noaa$snow))`, for snwd is `r sum(is.na(ny_noaa$snwd))`, for tmax is `r sum(is.na(ny_noaa$tmax))`, and for tmin is `r sum(is.na(ny_noaa$tmin))`.

Now, we attempt to clean the data set. In this step, we create separate variables for year, month, and day; modify units for temperature, precipitation, and snowfall such that they are reasonable.

```{r}
ny_noaa <- ny_noaa %>% 
  mutate(date = as.character(date),
         tmax = as.numeric(tmax),
         tmin = as.numeric(tmin)) %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    tmax = tmax * 0.1,
    tmin = tmin * 0.1,
    prcp = prcp * 0.1)
```

In this way, the unit for tmax and tmin are in degree C, and the units for prcp, snow, and snwd are all in mm. We now try to show the most commonly observed value for snowfall by writing a find_mode function as shown below.

```{r}
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}
```

In this way, we know that the most commonly observed value for snowfall is `r find_mode(ny_noaa$snow)`. Next, we make a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r}
tmax_jan <- 
  ny_noaa %>%
  filter(month == 1) %>% 
  group_by(year) %>% 
  summarize(tmax_mean = mean(tmax, na.rm=TRUE)) %>%
  ggplot(aes(x = year, y = tmax_mean)) +
  geom_point()

tmax_jul <- 
  ny_noaa %>%
  filter(month == 7) %>% 
  group_by(year) %>% 
  summarize(tmax_mean = mean(tmax, na.rm=TRUE)) %>%
  ggplot(aes(x = year, y = tmax_mean)) +
  geom_point()

tmax_jan + tmax_jul
```

It seems like the average max temperature in a year follows very roughly linearly for both January and July, but there due to the scattered nature of both of these structures, it is difficult to conclude whether the outstanding observations are outliers or if the distribution is in fact random.

Finally, we make a two-panel plot showing 1) tmax vs. tmin for the full data set and 2) the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
tmax_vs_tmin <- 
  ny_noaa %>%
  select(tmin, tmax) %>%
  drop_na() %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex()

snowf <- 
  ny_noaa %>% 
  snow = as.numeric(snow) %>%
  filter(snow > 0 & snow < 100) %>%
  group_by(year) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_point()

tmax_vs_min + snowf
```
